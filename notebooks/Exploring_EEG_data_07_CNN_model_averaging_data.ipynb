{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out CNN on averaged EEG data\n",
    "\n",
    "## Pre-processing\n",
    "+ Import data.\n",
    "+ Apply filters (bandpass).\n",
    "+ Detect potential bad channels and replace them by interpolation.\n",
    "+ Detect potential bad epochs and remove them.\n",
    "+ Average over a number of randomly drawn epochs (of same person and same stimuli).\n",
    "\n",
    "## Train CNN network\n",
    "+ Define network architecture\n",
    "+ Split data\n",
    "+ Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages & links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "#%matplotlib inline\n",
    "\n",
    "from mayavi import mlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"C:\\\\OneDrive - Netherlands eScience Center\\\\Project_ePodium\\\\\"\n",
    "PATH_CODE = ROOT + \"EEG_explorer\\\\\"\n",
    "PATH_DATA = ROOT + \"Data\\\\\"\n",
    "PATH_OUTPUT = ROOT + \"Data\\\\processed\\\\\"\n",
    "PATH_METADATA = ROOT + \"Data\\\\metadata\\\\\"\n",
    "PATH_MODELS = ROOT + \"trained_models\\\\\"\n",
    "file_labels = \"metadata.xlsx\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, PATH_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed dataset\n",
    "+ See notebook for preprocessing: Exploring_EEG_data_04_prepare_data_for_ML.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PATH_OUTPUT + \"EEG_data_30channels_1s_corrected.npy\"\n",
    "signal_collection = np.load(filename)\n",
    "\n",
    "filename = PATH_OUTPUT + \"EEG_data_30channels_1s_corrected_labels.npy\"\n",
    "label_collection = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "metadata = []\n",
    "filename = PATH_OUTPUT + \"EEG_data_30channels_1s_corrected_metadata.csv\"\n",
    "with open(filename, 'r') as readFile:\n",
    "    reader = csv.reader(readFile, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) > 0:\n",
    "            metadata.append(row)\n",
    "readFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '034_17_mc_mmn36_wk.cnt', '443'],\n",
       " ['2', '036_17_mc_mmn36_wk.cnt', '1384'],\n",
       " ['18', '175_17_jd_mmn_wk.cnt', '1858'],\n",
       " ['26', '305_17_jc_mmn36_wk.cnt', '2349'],\n",
       " ['27', '306_17_mc_mmn36_wk.cnt', '2813'],\n",
       " ['28', '307_17_jc_mmn36_wakker.cnt', '3447'],\n",
       " ['29', '308_17_jc_mmn36_wk.cnt', '3938'],\n",
       " ['30', '309_17_jc_mmn.cnt', '4425'],\n",
       " ['33', '314_17_mc_mmn36_wk.cnt', '4917'],\n",
       " ['38', '337_17_jc_mmn36_wk.cnt', '5389']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data --> Scale/normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_maxes = []\n",
    "for ID in range(1,len(metadata)):\n",
    "    min_maxes.append((signal_collection[int(metadata[ID-1][2]):int(metadata[ID][2]),:,:].min(), \n",
    "                     signal_collection[int(metadata[ID-1][2]):int(metadata[ID][2]),:,:].max(), \n",
    "                     signal_collection[int(metadata[ID-1][2]):int(metadata[ID][2]),:,:].mean(), \n",
    "                     signal_collection[int(metadata[ID-1][2]):int(metadata[ID][2]),:,:].var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = np.mean([x[1] for x in min_maxes]) #signal_collection.max()\n",
    "data_min = np.mean([x[0] for x in min_maxes]) #signal_collection.min()\n",
    "data_mean = signal_collection.mean()\n",
    "\n",
    "signal_collection = signal_collection - data_mean\n",
    "signal_collection = signal_collection / data_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.9539306399697487, 2.2628056343165652)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_collection.min(), signal_collection.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make clearer disctinction between training, test, validation data!\n",
    "To be sure the network is able to make predictions using unseen data, the dataset could be split according to persons! Unfortunately we only have data here for 57 persons (24 in group 1 and 33 in group 2).  \n",
    "This makes this approach a bit complicated\n",
    "\n",
    "## Idea for next steps:\n",
    "We could also loop through different test-person/train-person splits and independently train models on those datasets. Then, in the the end the outcome would be averaged over all of those. This way we would make better use of the data we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "for ID in range(len(metadata)):\n",
    "    if ID == 0:\n",
    "        low = 0\n",
    "    else:\n",
    "        low = int(metadata[ID-1][2])\n",
    "        \n",
    "    groups.append(int(label_collection[low]/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_ids = np.where(np.array(groups) == 1)[0]\n",
    "group_2_ids = np.where(np.array(groups) == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 31, 32, 33, 34, 35, 36,\n",
       "       37, 38, 39, 40, 41, 42, 43], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect group 2 ID's\n",
    "group_2_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_averaged_data(signal_collection, \n",
    "                         label_collection, \n",
    "                         metadata, \n",
    "                         n_epochs, \n",
    "                         selected_ids):\n",
    "    \"\"\" \n",
    "    Function to create averages of n_epochs epochs.\n",
    "    \n",
    "    \"\"\"\n",
    "    # create new data collection:\n",
    "    X_data = np.zeros((0, signal_collection.shape[1], signal_collection.shape[2]))\n",
    "    y_data = []\n",
    "    \n",
    "    for ID in selected_ids:\n",
    "        print(\"person ID:\", ID, \". Originally from file: \", metadata[ID][1])\n",
    "        if ID == 0:\n",
    "            low = 0\n",
    "        else:\n",
    "            low = int(metadata[ID-1][2])\n",
    "        high = int(metadata[ID][2])\n",
    "\n",
    "        # Create class=3 averages (n_class3_per_patient times)\n",
    "        for i in range(n_class3_per_patient):      \n",
    "            select_label = np.where(label_collection[low:high] == 3)[0]\n",
    "            if len(select_label) == 0:\n",
    "                select_label = np.where(label_collection[low:high] == 6)[0]\n",
    "                group = 2\n",
    "            else:\n",
    "                group = 1\n",
    "            if len(select_label) >= n_epochs:        \n",
    "                select = np.random.choice(select_label, n_epochs, replace=False)\n",
    "            else:\n",
    "                print(\"Found only\", len(select_label), \" epochs and will take those!\")\n",
    "                signal_averaged = np.mean(signal_collection[select_label,:,:], axis=0)\n",
    "                break\n",
    "            signal_averaged = np.mean(signal_collection[select,:,:], axis=0)\n",
    "            X_data = np.concatenate([X_data, np.expand_dims(signal_averaged, axis=0)], axis=0)\n",
    "            y_data.append(group*3)\n",
    "\n",
    "         # Create class=13 averages (n_class13_per_patient times)\n",
    "        for i in range(n_class13_per_patient):      \n",
    "            select_label = np.where(label_collection[low:high] == 13)[0]\n",
    "            if len(select_label) == 0:\n",
    "                select_label = np.where(label_collection[low:high] == 26)[0]\n",
    "                group = 2\n",
    "            else:\n",
    "                group = 1\n",
    "            if len(select_label) >= n_epochs:        \n",
    "                select = np.random.choice(select_label, n_epochs, replace=False)\n",
    "            else:\n",
    "                print(\"Found only\", len(select_label), \" epochs and will take those!\")\n",
    "                signal_averaged = np.mean(signal_collection[select_label,:,:], axis=0)\n",
    "                break\n",
    "            signal_averaged = np.mean(signal_collection[select,:,:], axis=0)\n",
    "            X_data = np.concatenate([X_data, np.expand_dims(signal_averaged, axis=0)], axis=0)\n",
    "            y_data.append(group*13)\n",
    "\n",
    "        # Create class=66 averages (n_class66_per_patient times)\n",
    "        for i in range(n_class66_per_patient):      \n",
    "            select_label = np.where(label_collection[low:high] == 66)[0]\n",
    "            if len(select_label) == 0:\n",
    "                select_label = np.where(label_collection[low:high] == 132)[0]\n",
    "                group = 2\n",
    "            else:\n",
    "                group = 1\n",
    "            if len(select_label) >= n_epochs:        \n",
    "                select = np.random.choice(select_label, n_epochs, replace=False)\n",
    "            else:\n",
    "                print(\"Found only\", len(select_label), \" epochs and will take those!\")\n",
    "                signal_averaged = np.mean(signal_collection[select_label,:,:], axis=0)\n",
    "                break\n",
    "            signal_averaged = np.mean(signal_collection[select,:,:], axis=0)\n",
    "            X_data = np.concatenate([X_data, np.expand_dims(signal_averaged, axis=0)], axis=0)\n",
    "            y_data.append(group*66)\n",
    "\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 30 # Average over n_epochs epochs\n",
    "n_class3_per_patient = 20\n",
    "n_class13_per_patient = 10\n",
    "n_class66_per_patient = 10\n",
    "\n",
    "# Make selection \n",
    "group_1_ids = np.where(np.array(groups) == 1)[0]\n",
    "group_2_ids = np.where(np.array(groups) == 2)[0]\n",
    "keep_group1_for_test = 10\n",
    "keep_group2_for_test = 10\n",
    "\n",
    "# Initialize random numbers to get reproducible results \n",
    "np.random.seed(42)\n",
    "\n",
    "# Make random selection\n",
    "selected_ids_test = [x for x in np.concatenate([np.random.choice(group_1_ids, keep_group1_for_test),\n",
    "                                   np.random.choice(group_2_ids, keep_group2_for_test)])]\n",
    "\n",
    "selected_ids = [x for x in range(len(metadata)) if not x in selected_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person ID: 0 . Originally from file:  034_17_mc_mmn36_wk.cnt\n",
      "person ID: 1 . Originally from file:  036_17_mc_mmn36_wk.cnt\n",
      "person ID: 2 . Originally from file:  175_17_jd_mmn_wk.cnt\n",
      "person ID: 3 . Originally from file:  305_17_jc_mmn36_wk.cnt\n",
      "person ID: 4 . Originally from file:  306_17_mc_mmn36_wk.cnt\n",
      "person ID: 5 . Originally from file:  307_17_jc_mmn36_wakker.cnt\n",
      "person ID: 6 . Originally from file:  308_17_jc_mmn36_wk.cnt\n",
      "person ID: 9 . Originally from file:  337_17_jc_mmn36_wk.cnt\n",
      "person ID: 10 . Originally from file:  343_17_mc_mmm36_wk.cnt\n",
      "person ID: 14 . Originally from file:  420_17_md_mmn.cnt\n",
      "person ID: 15 . Originally from file:  428_17_md_mmn.cnt\n",
      "person ID: 16 . Originally from file:  434_17_jd_mmn.cnt\n",
      "person ID: 17 . Originally from file:  435_17_md_mmn36_wk.cnt\n",
      "person ID: 18 . Originally from file:  436_17_jd_mmn36_wk.cnt\n",
      "person ID: 19 . Originally from file:  443_17_jd_mmn36_wk.cnt\n",
      "person ID: 20 . Originally from file:  454_17_md_mmn36_wk.cnt\n",
      "Found only 27  epochs and will take those!\n",
      "person ID: 22 . Originally from file:  466_17_md_mmn36_wk.cnt\n",
      "person ID: 23 . Originally from file:  472_17_jd_mmn36_wk.cnt\n",
      "person ID: 24 . Originally from file:  475_17_jd_mmn36_wk.cnt\n",
      "person ID: 25 . Originally from file:  478_17_jd_mmn_36_wk.cnt\n",
      "person ID: 26 . Originally from file:  482_17_md_mmn36_wk.cnt\n",
      "person ID: 27 . Originally from file:  487_17_md_mmn_36_wk.cnt\n",
      "person ID: 28 . Originally from file:  488_17_jd_mmn36_wk.cnt\n",
      "person ID: 29 . Originally from file:  489_17_md_mmn36_wk.cnt\n",
      "person ID: 30 . Originally from file:  491_17_jd_mmn36_wk.cnt\n",
      "person ID: 31 . Originally from file:  602-115-17m-mc-mmn.cnt\n",
      "person ID: 32 . Originally from file:  604-133-17m-jc-mmn36.cnt\n",
      "person ID: 33 . Originally from file:  605-131-17m-jc-mmn.cnt\n",
      "person ID: 35 . Originally from file:  611_157_17m_mc_mmn36.cnt\n",
      "person ID: 36 . Originally from file:  619-247-17m-mc-mmn36.cnt\n",
      "person ID: 37 . Originally from file:  633-403-17m-jc-mmn36.cnt\n",
      "person ID: 41 . Originally from file:  640-464-17m-jc-mmn36.cnt\n",
      "Found only 21  epochs and will take those!\n",
      "Found only 21  epochs and will take those!\n",
      "person ID: 43 . Originally from file:  646-478-17m-mc-mmn36.cnt\n",
      "person ID: 46 . Originally from file:  720-166-17m-jr-mmn.cnt\n",
      "person ID: 48 . Originally from file:  725_161_17m_jr_mmn36.cnt\n",
      "person ID: 49 . Originally from file:  726_126_17m-jr_mmn36.cnt\n",
      "person ID: 50 . Originally from file:  730-201-17m-jr-mmn.cnt\n",
      "person ID: 51 . Originally from file:  739-368-17m-mr-mmn36.cnt\n",
      "person ID: 52 . Originally from file:  749-461-17m-jr-mmn36.cnt\n",
      "person ID: 54 . Originally from file:  754-472-17m-jr-mmn36.cnt\n",
      "person ID: 55 . Originally from file:  757-487-17m-jr-mmn36.cnt\n",
      "person ID: 56 . Originally from file:  758-465-17m-mr-mmn36.cnt\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = create_averaged_data(signal_collection, \n",
    "                         label_collection, \n",
    "                         metadata, \n",
    "                         n_epochs, \n",
    "                         selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person ID: 7 . Originally from file:  309_17_jc_mmn.cnt\n",
      "person ID: 39 . Originally from file:  637-479-17m-mc-mmn36.cnt\n",
      "person ID: 34 . Originally from file:  609-158-17m-jc-mmn36.cnt\n",
      "person ID: 11 . Originally from file:  345_17_mc_mmn36_wk.cnt\n",
      "person ID: 8 . Originally from file:  314_17_mc_mmn36_wk.cnt\n",
      "person ID: 40 . Originally from file:  639-484-17m-jc-mmn36.cnt\n",
      "person ID: 7 . Originally from file:  309_17_jc_mmn.cnt\n",
      "person ID: 38 . Originally from file:  636-468-17m-jc-mmn36.cnt\n",
      "person ID: 42 . Originally from file:  642-485-17m-jc-mmn36.cnt\n",
      "person ID: 11 . Originally from file:  345_17_mc_mmn36_wk.cnt\n",
      "person ID: 21 . Originally from file:  455_17_jd_mmn36_wk.cnt\n",
      "person ID: 47 . Originally from file:  724-116-17m-jr-mmn36.cnt\n",
      "person ID: 47 . Originally from file:  724-116-17m-jr-mmn36.cnt\n",
      "person ID: 13 . Originally from file:  413_17_jd_mmn_2.cnt\n",
      "person ID: 45 . Originally from file:  719-079-jr-17m-mmn36.cnt\n",
      "person ID: 12 . Originally from file:  406_17_md_mmn.cnt\n",
      "person ID: 47 . Originally from file:  724-116-17m-jr-mmn36.cnt\n",
      "person ID: 53 . Originally from file:  751-542-17m-jr-mmn36.cnt\n",
      "person ID: 12 . Originally from file:  406_17_md_mmn.cnt\n",
      "person ID: 44 . Originally from file:  707-060-17m-jd-mmn.cnt\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = create_averaged_data(signal_collection, \n",
    "                         label_collection, \n",
    "                         metadata, \n",
    "                         n_epochs, \n",
    "                         selected_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 30, 501) 1650\n",
      "(800, 30, 501) 800\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape, len(y_data))\n",
    "print(X_test.shape, len(y_test))\n",
    "#print(y_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmean = np.concatenate([X_data, X_test]).mean()\n",
    "X_data = X_data - Xmean\n",
    "X_test = X_test - Xmean\n",
    "\n",
    "Xmax = np.concatenate([X_data, X_test]).max()\n",
    "\n",
    "X_data = X_data / Xmax\n",
    "X_test = X_test / Xmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.002006264229741046, 0.9934850700415665, -1.13929890512535)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.mean(), X_data.max(),X_data.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data\n",
    "### Now the test dataset is already entirely seperate from the rest! \n",
    "+ validation data, used to monitor the model progress and avoid overfitting.\n",
    "+ testing data, meant for final check on model performance.\n",
    "+ --> Create validation and test data set from seperated data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "X_train, y_train = shuffle(X_data, y_data, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1650\n",
      "Validation set size: 400\n",
      "Test set size: 400\n",
      "\n",
      "X_train mean, min, max:  -0.0020062642297410485 -1.13929890512535 0.9934850700415665\n"
     ]
    }
   ],
   "source": [
    "print('Train set size:', X_train.shape[0])\n",
    "print('Validation set size:', X_val.shape[0])\n",
    "print('Test set size:', X_test.shape[0])\n",
    "print()\n",
    "print(\"X_train mean, min, max: \", np.mean(X_train), np.min(X_train), np.max(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to 1-hot encoding for labels\n",
    "+ We have six categories or classes. Those are best represented by a so called 1-hot encoding. This means nothing else than simply a binary 0-or-1 for every class.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_transform = LabelBinarizer()\n",
    "\n",
    "y_train_binary = label_transform.fit_transform(np.array(y_train).astype(int))\n",
    "y_val_binary = label_transform.fit_transform(np.array(y_val).astype(int))\n",
    "y_test_binary = label_transform.fit_transform(np.array(y_test).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_binary[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   6,  13,  26,  66, 132])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show found labels:\n",
    "label_transform.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution accross the 6 label categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.193939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.315152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.157576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.151515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frequency\n",
       "3     0.193939\n",
       "6     0.315152\n",
       "13    0.090909\n",
       "26    0.157576\n",
       "66    0.090909\n",
       "132   0.151515"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(label_transform.classes_)\n",
    "frequencies = y_train_binary.mean(axis=0)\n",
    "frequencies_df = pd.DataFrame(frequencies, index=labels, columns=['frequency'])\n",
    "frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "We have more data on group 2 than on group 1. And far more data for stimuli 3 than for stimuli 13 and 66 (not surprising). \n",
    "\n",
    "--> post on balancing datasets: https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n",
    "\n",
    "### Needs some thinking on how to balance the data set !\n",
    "e.g. by frequency dependend selection rule, or by defining a suitied special loss function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                               ,np.unique(y_train)\n",
    "                                               ,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.859375  , 0.52884615, 1.83333333, 1.05769231, 1.83333333,\n",
       "       1.1       ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: class_weight[0],\n",
    "               1: class_weight[1],\n",
    "               2: class_weight[2],\n",
    "               3: class_weight[3],\n",
    "               4: class_weight[4],\n",
    "               5: class_weight[5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "output_file = 'CNN_EEG_classifier_avg_02'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = PATH_MODELS + output_file + \".hdf5\", monitor='val_acc', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_acc', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN model\n",
    "n_timesteps = 501\n",
    "n_features = 30\n",
    "n_outputs = 6\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=20, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(layers.AveragePooling1D(pool_size=2))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=10, activation='relu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=2))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=2))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(layers.AveragePooling1D(pool_size=2))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 482, 32)           19232     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_8 (Average (None, 241, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 232, 64)           20544     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_9 (Average (None, 116, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 112, 128)          41088     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_10 (Averag (None, 56, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 54, 128)           49280     \n",
      "_________________________________________________________________\n",
      "average_pooling1d_11 (Averag (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                172850    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 303,300\n",
      "Trainable params: 303,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1650 samples, validate on 400 samples\n",
      "WARNING:tensorflow:From C:\\Users\\FlorianHuber\\Anaconda3\\envs\\mne\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.9217 - acc: 0.2102\n",
      "Epoch 00001: val_acc improved from -inf to 0.20750, saving model to C:\\OneDrive - Netherlands eScience Center\\Project_ePodium\\trained_models\\CNN_EEG_classifier_avg_02.hdf5\n",
      "1650/1650 [==============================] - 18s 11ms/sample - loss: 1.9195 - acc: 0.2097 - val_loss: 1.6885 - val_acc: 0.2075\n",
      "Epoch 2/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.6247 - acc: 0.3039\n",
      "Epoch 00002: val_acc improved from 0.20750 to 0.35750, saving model to C:\\OneDrive - Netherlands eScience Center\\Project_ePodium\\trained_models\\CNN_EEG_classifier_avg_02.hdf5\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.6267 - acc: 0.3030 - val_loss: 1.4658 - val_acc: 0.3575\n",
      "Epoch 3/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.4830 - acc: 0.3578\n",
      "Epoch 00003: val_acc did not improve from 0.35750\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.4815 - acc: 0.3588 - val_loss: 1.4224 - val_acc: 0.3500\n",
      "Epoch 4/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.3925 - acc: 0.3940\n",
      "Epoch 00004: val_acc improved from 0.35750 to 0.37750, saving model to C:\\OneDrive - Netherlands eScience Center\\Project_ePodium\\trained_models\\CNN_EEG_classifier_avg_02.hdf5\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.3933 - acc: 0.3939 - val_loss: 1.3409 - val_acc: 0.3775\n",
      "Epoch 5/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.2839 - acc: 0.4277\n",
      "Epoch 00005: val_acc improved from 0.37750 to 0.41250, saving model to C:\\OneDrive - Netherlands eScience Center\\Project_ePodium\\trained_models\\CNN_EEG_classifier_avg_02.hdf5\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.2813 - acc: 0.4279 - val_loss: 1.2369 - val_acc: 0.4125\n",
      "Epoch 6/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.2137 - acc: 0.4602\n",
      "Epoch 00006: val_acc did not improve from 0.41250\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.2147 - acc: 0.4576 - val_loss: 1.3652 - val_acc: 0.3775\n",
      "Epoch 7/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.1706 - acc: 0.4461\n",
      "Epoch 00007: val_acc did not improve from 0.41250\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.1712 - acc: 0.4461 - val_loss: 1.1397 - val_acc: 0.3725\n",
      "Epoch 8/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.1170 - acc: 0.4859\n",
      "Epoch 00008: val_acc did not improve from 0.41250\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.1159 - acc: 0.4842 - val_loss: 1.1734 - val_acc: 0.3900\n",
      "Epoch 9/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.0818 - acc: 0.4767\n",
      "Epoch 00009: val_acc did not improve from 0.41250\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.0845 - acc: 0.4764 - val_loss: 1.1548 - val_acc: 0.4075\n",
      "Epoch 10/50\n",
      "1632/1650 [============================>.] - ETA: 0s - loss: 1.0334 - acc: 0.5123\n",
      "Epoch 00010: val_acc did not improve from 0.41250\n",
      "1650/1650 [==============================] - 16s 10ms/sample - loss: 1.0299 - acc: 0.5145 - val_loss: 1.1684 - val_acc: 0.3875\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c76248bcf8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# fit network\n",
    "model.fit(np.swapaxes(X_train,1,2), \n",
    "          y_train_binary, \n",
    "          validation_data=(np.swapaxes(X_val,1,2), y_val_binary), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          class_weight = class_weight,\n",
    "          callbacks = [checkpointer, earlystopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seems to overfit on the training data and not be able to predict well the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "_, train_acc = model.evaluate(np.swapaxes(X_train,1,2), y_train_binary, verbose=0)\n",
    "_, test_acc = model.evaluate(np.swapaxes(X_test,1,2), y_test_binary, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train dataset: 0.5478788\n",
      "Accuracy on test dataset: 0.3925\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on train dataset:\", train_acc)\n",
    "print(\"Accuracy on test dataset:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.swapaxes(X_test,1,2)\n",
    "\n",
    "# Check model predictions:\n",
    "y_pred_proba = model.predict_proba(Xtest)\n",
    "y_pred_classes = model.predict_classes(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 0 1 0 1 0 5 0 1 0 4 0 2 0 1 5 1 2 0]\n",
      "[0 5 0 0 0 0 4 5 0 0 0 4 0 5 2 0 5 0 4 3]\n"
     ]
    }
   ],
   "source": [
    "y_test_05 = np.array(y_test.copy())\n",
    "y_test_05[y_test_05 == 132] = 5\n",
    "y_test_05[y_test_05 == 66] = 4\n",
    "y_test_05[y_test_05 == 26] = 3\n",
    "y_test_05[y_test_05 == 13] = 2\n",
    "y_test_05[y_test_05 == 6] = 1\n",
    "y_test_05[y_test_05 == 3] = 0\n",
    "\n",
    "print(y_test_05[:20].astype(int))\n",
    "print(y_pred_classes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test_05, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if groups are predicted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 2 1 2 2 2 1 2 1 1 1 1 2 2 2 2 1 1 1 2 1 1 2 1 2 1 2 1]\n",
      "[1 2 1 1 1 1 2 2 1 1 1 2 1 2 2 1 2 1 2 2 2 1 2 2 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "y_test_12 = np.array(y_test.copy())\n",
    "y_test_12[y_test_12 == 132] = 2\n",
    "y_test_12[y_test_12 == 66] = 1\n",
    "y_test_12[y_test_12 == 26] = 2\n",
    "y_test_12[y_test_12 == 13] = 1\n",
    "y_test_12[y_test_12 == 6] = 2\n",
    "y_test_12[y_test_12 == 3] = 1\n",
    "\n",
    "y_pred_12 = y_pred_classes.copy()\n",
    "y_pred_12[y_pred_12 == 5] = 2\n",
    "y_pred_12[y_pred_12 == 4] = 1\n",
    "y_pred_12[y_pred_12 == 3] = 2\n",
    "y_pred_12[y_pred_12 == 2] = 1\n",
    "y_pred_12[y_pred_12 == 1] = 2\n",
    "y_pred_12[y_pred_12 == 0] = 1\n",
    "\n",
    "print(y_test_12[:30].astype(int))\n",
    "print(y_pred_12[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.465"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_12 == y_pred_12)/ y_pred_12.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "So far this is not working as a proper discriminator between group1 and group2!  \n",
    "The test dataset contains about equal amounts of data from both groups, and the model does do better than random guessing (50% hits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71, 19, 33, 11, 10,  3],\n",
       "       [87, 12,  0,  1,  1,  0],\n",
       "       [ 3,  3, 20, 17,  4,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  5,  2, 16, 25],\n",
       "       [ 1,  0,  8,  6,  6, 31]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix:\n",
    "M_confusion = metrics.confusion_matrix(y_test_05, y_pred_classes)\n",
    "M_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 162)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_05 == 0), np.sum(y_pred_classes == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interestingly, it might be that the model at least does see a difference between the types of stimuli!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 1 1 1 2 2 1 1 1 2 1 2 2 1 2 1 2 1 2 1 2 1 2 1 2 2 2 2]\n",
      "[1 2 1 1 1 1 2 2 1 1 1 2 1 2 2 1 2 1 2 2 2 1 2 1 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "y_test_bak_dak = np.array(y_test.copy())\n",
    "y_test_bak_dak[y_test_bak_dak == 132] = 2\n",
    "y_test_bak_dak[y_test_bak_dak == 66] = 2\n",
    "y_test_bak_dak[y_test_bak_dak == 26] = 2\n",
    "y_test_bak_dak[y_test_bak_dak == 13] = 2\n",
    "y_test_bak_dak[y_test_bak_dak == 6] = 1\n",
    "y_test_bak_dak[y_test_bak_dak == 3] = 1\n",
    "\n",
    "y_pred_bak_dak = y_pred_classes.copy()\n",
    "y_pred_bak_dak[y_pred_bak_dak == 5] = 2\n",
    "y_pred_bak_dak[y_pred_bak_dak == 4] = 2\n",
    "y_pred_bak_dak[y_pred_bak_dak == 3] = 2\n",
    "y_pred_bak_dak[y_pred_bak_dak == 2] = 2\n",
    "y_pred_bak_dak[y_pred_bak_dak == 1] = 1\n",
    "y_pred_bak_dak[y_pred_bak_dak == 0] = 1\n",
    "\n",
    "print(y_test_bak_dak[:30].astype(int))\n",
    "print(y_pred_bak_dak[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_bak_dak == y_pred_bak_dak)/ y_pred_bak_dak.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interestingly though,...\n",
    "That would mean that the model is correct in 96% of all cases in distinguishing 'bak' from 'dak'.  \n",
    "--> **Careful: Needs to be checked if my assumptions about the stimuli are correct...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
